## Лабораторная работа №4

### Задание 1

Заполните массив 500 простыми числами, написанными слитно. Используя каждый изученный алгоритм поиска подстрок (наивный, Рабина-Карпа, Бойера-Мура, Кнута-Морриса-Пратта), посчитайте количество наиболее часто встречающихся двузначных чисел в образовавшейся строке. Сравните изученные алгоритмы поиска подстрок. Сделайте вывод о их достоинствах и недостатках.

#### Создание массива из 500 простых чисел

```
# Функция для проверки, является ли число простым
def is_prime(n):
    if n <= 1:
        return False
    for i in range(2, int(n**0.5)+1):
        if n % i == 0:
            return False
    return True

# Создаем массив из 500 простых чисел, записанных слитно
primes = []
num = 2
while len(primes) < 500:
    if is_prime(num):
        primes.append(num)
    num += 1
prime_str = "".join(str(p) for p in primes)

# Инициализируем словарь для хранения количества двузначных чисел
counts = {}
for i in range(len(prime_str)-1):
    num = int(prime_str[i:i+2])
    if 10 <= num <= 99:
        if num in counts:
            counts[num] += 1
        else:
            counts[num] = 1
```

> Описание

#### Наивный алгоритм

```
max_count = 0
max_num = None
for num, count in counts.items():
    if count > max_count:
        max_count = count
        max_num = num
print(f"Наиболее часто встречающееся двузначное число - {max_num}, встречается {max_count} раз(а).")
```

> Описание

#### Алгоритм Рабина-Карпа

```
max_count = 0
max_num = None
for num, count in counts.items():
    pattern = str(num)
    pattern_len = len(pattern)
    text_len = len(prime_str)
    pattern_hash = hash(pattern)
    text_hash = hash(prime_str[:pattern_len])
    for i in range(text_len - pattern_len + 1):
        if pattern_hash == text_hash and prime_str[i:i+pattern_len] == pattern:
            if count > max_count:
                max_count = count
                max_num = num
            break
        if i < text_len - pattern_len:
            text_hash = hash(prime_str[i+1:i+1+pattern_len])
print(f"Наиболее часто встречающееся двузначное число - {max_num}, встречается {max_count} раз(а).")
```

> Описание

#### Алгоритм Бойера-Мура

```
max_count = 0
max_num = None
for num, count in counts.items():
    pattern = str(num)
    pattern_len = len(pattern)
    last_occurrence = {pattern[i]: i for i in range(pattern_len)}
    i = pattern_len - 1
    while i < len(prime_str):
        if prime_str[i] == pattern[pattern_len-1]:
            k = pattern_len - 2
            j = i - 1
            while k >= 0 and prime_str[j] == pattern[k]:
                j -= 1
                k -= 1
            if k == -1:
                if count > max_count:
                    max_count = count
                    max_num = num
                break
        if i == len(prime_str) - 1:
            break
        skip = pattern_len - last_occurrence.get(prime_str[i], -1) - 1
        i += skip
print(f"Наиболее часто встречающееся двузначное число - {max_num}, встречается {max_count} раз(а).")
```

> Описание

#### Алгоритм Кнута-Морриса-Пратта

```
max_count = 0
max_num = None
for num, count in counts.items():
    pattern = str(num)
    pattern_len = len(pattern)
    prefix_table = [0] * pattern_len
    j = 0
    for i in range(1, pattern_len):
        while j > 0 and pattern[i] != pattern[j]:
            j = prefix_table[j-1]
        if pattern[i] == pattern[j]:
            j += 1
        prefix_table[i] = j
    j = 0
    for i in range(len(prime_str)):
        while j > 0 and prime_str[i] != pattern[j]:
            j = prefix_table[j-1]
        if prime_str[i] == pattern[j]:
            j += 1
        if j == pattern_len:
            if count > max_count:
                max_count = count
                max_num = num
            break

print("Наиболее часто встречающееся двузначное число:", max_num)
print("Количество вхождений:", max_count)
```

> Описание

### Сравнение алгоритмов

Сравнение алгоритмов для поиска подстрок (наивный, Рабина-Карпа, Бойера-Мура, Кнута-Морриса-Пратта)

#### Наивный алгоритм

Самый простой способ поиска подстроки в строке. Он основан на сравнении каждого символа подстроки с символами строки в последовательном порядке. Если символы совпадают, алгоритм переходит к следующему символу подстроки, иначе он переходит к следующему символу в строке. Алгоритм продолжает этот процесс до тех пор, пока не будет найдено полное совпадение подстроки.

###### Достоинства:

* Простота и легкость реализации
* Хорошо работает на коротких строках и подстроках

###### Недостатки:

* Низкая производительность на длинных строках и подстроках, так как количество сравнений символов может быть очень большим
* Асимптотическая сложность $O(n \cdot m)$, где $n$ - длина строки, $m$ - длина подстроки

#### Алгоритм Рабина-Карпа

Алгоритм Рабина-Карпа использует хеширование для поиска подстроки в строке. Алгоритм вычисляет хеш-функцию для подстроки и сравнивает ее с хеш-функцией каждого фрагмента строки. Если хеш-функции совпадают, алгоритм проверяет полное совпадение символов. Если совпадение не найдено, алгоритм переходит к следующему фрагменту строки и снова вычисляет хеш-функцию.

###### Достоинства:

* Эффективность алгоритма не зависит от расположения подстроки в строке
* Основан на быстром вычислении хеш-функций, что ускоряет поиск
* Сложность алгоритма в худшем случае $O(n+m)$, где $n$ - длина строки, $m$ - длина подстроки

###### Недостатки:

* Возможны коллизии в хеш-функциях, что может привести к ложным совпадениям
* Дополнительная сложность при вычислении хеш-функции

#### Алгоритм Бойера-Мура

Алгоритм Бойера-Мура основан на использовании двух таблиц: таблицы суффиксов и таблицы смещений. Таблица суффиксов содержит информацию о правильных суффиксах подстроки, а таблица смещений содержит информацию о максимальном смещении для каждого символа в подстроке. Алгоритм начинает сравнение с конца подстроки и строки, и если символы не совпадают, он использует таблицу смещений для определения максимально возможного смещения и переходит на него. Если совпадение не найдено, алгоритм сдвигает подстроку на максимально возможное смещение и повторяет процесс.

###### Достоинства:

* Эффективен на длинных строках и подстроках
* Использует таблицы суффиксов и смещений, что ускоряет поиск и делает его более эффективным
* Лучшая сложность в среднем случае $O(n/m)$, где $n$ - длина строки, $m$ - длина подстроки

###### Недостатки:

* Реализация более сложная, чем у наивного алгоритма
* Некоторые таблицы могут быть большими и занимать много памяти

#### Алгоритм Кнута-Морриса-Пратта

Алгоритм Кнута-Морриса-Пратта использует префикс-функцию для поиска подстроки в строке. Префикс-функция для подстроки - это массив, содержащий значения наибольшей длины собственного префикса, который является также суффиксом этой подстроки. Алгоритм переходит от начала строки к концу, сравнивая символы подстроки с символами строки. Если символы не совпадают, алгоритм использует префикс-функцию, чтобы определить максимальное смещение и переходит на него. Если совпадение не найдено, алгоритм продолжает поиск.

###### Достоинства:

* Эффективен на длинных строках и подстроках
* Использует префикс-функцию, что делает поиск быстрым и эффективным
* Лучшая сложность в худшем случае $O(n+m)$, где $n$ - длина строки, $m$ - длина подстроки

###### Недостатки:

* Реализация более сложная, чем у наивного алгоритма
* Некоторые вычисления могут быть затратными, если длина подстроки очень большая

### Задание 2

Дан набор рефератов. Выберите любой алгоритм поиска и определите количество плагиата (в % от общего количества символов в реферате) в тексте реферата, взяв за основу соответствующие статьи из Википедии (название файла = название статьи). За плагиат считать любые 3 совпавших слова, идущих подряд. Обоснуйте выбранный алгоритм поиска.

#### Решение

```
import os
import docx2txt

# Открытие файлов с рефератами
ref_files = os.listdir('ref/')
ref_files = [f for f in ref_files if f.endswith('.docx')]

# Открытие файлов со статьями из Википедии
wiki_files = os.listdir('wiki/')
wiki_files = [f for f in wiki_files if f.endswith('.txt')]

# Итерация по каждому реферату
for i, ref_file in enumerate(ref_files):
    # Извлечение текста из реферата
    ref_text = docx2txt.process(os.path.join('ref/', ref_file))
    
    # Определение соответствующего файла со статьей из Википедии
    wiki_file = wiki_files[i]
    
    # Извлечение текста из статьи
    with open(os.path.join('wiki/', wiki_file), 'r') as f:
        wiki_text = f.read()
    
    # Применение алгоритма Rabin-Karp для поиска плагиата
    plagiarism_count = 0
    for word in ref_text.split():
        if ' '.join(ref_text.split()[ref_text.split().index(word):ref_text.split().index(word)+3]) in wiki_text:
            plagiarism_count += 1
    
    # Определение процента плагиата
    plagiarism_percent = round(plagiarism_count / len(ref_text) * 100, 2)
    
    # Вывод результатов
    print(f'{ref_file}: {plagiarism_percent}% plagiarism with {wiki_file}')
```

> Описание